---
title: Introduction
description: "Welcome to Openmind (OM1)"
---

Openmind's OM1 is an AI runtime for agents and robots with modular capabilities like movement, speech, and perception.

OM1 allows AI agents to be configured and then deployed in both digital and physical world. You can create *one* AI agent and run it in the cloud but also on physical robot hardware such as quadrupeds, and, soon, TurtleBot 3 and Humanoids. 

This means that one AI agent with a defined persona can ingest data from multiple sources (the web, X/Twitter, cameras, LIDAR, GPS, the NYSE, ...), and can then Tweet and explore your house, shake you hand, or talk to you. With OM1, you can talk with OpenAI's gpt-4o and literally shake hands with it.

Capabilities of OM1:

* Simple, Modular architecture
* All python 
* Easy to add new data inputs
* Easy to support new hardware, such as API endpoints and robots
* Can be connected to ROS2, Zenoh, and CycloneDDS
* Includes a simple web-based debug display to watch the system work 
* Preconfigured endpoints for Voice-to-Speech, OpenAI's `gpt-4o`, DeepSeek, and multiple VLMs

<CardGroup cols={2}>
  <Card
    title="Quick Start"
    icon="check"
    href="./quick_start"
  >
    Get started with Openmind OS (OM1)
  </Card>
  <Card
    title="Developer Guide"
    icon="code"
    href="./development/guide"
  >
    Developer Guide for Openmind OS (OM1)
  </Card>
  <Card
    title="Unitree Robotics"
    icon="dog"
    href="./robotics/unitree_robotics"
  >
    Run OM1 on a Unitree Dog
  </Card>
</CardGroup>