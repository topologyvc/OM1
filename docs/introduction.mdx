---
title: Introduction
description: "Welcome to Openmind (OM1)"
---

Openmind's OM1 is a modular AI runtime for agents and robots with capabilities including movement and speech.

OM1 allows AI agents to be configured and then deployed in both the digital and physical world. You can create *one* AI agent and run it in the cloud but also on physical robot hardware such as Quadrupeds, and, soon, TurtleBot 3 and Humanoids. 

For example, an AI agent built on OM1 can ingest data from multiple sources (the web, X/Twitter, cameras, and LIDAR) and can then Tweet and explore your house, shake your hand, or talk to you. In another example, with OM1, you can talk with OpenAI's `gpt-4o` and literally shake hands with it.

Capabilities of OM1:

* Simple, modular architecture
* All python 
* Easy to add new data inputs
* Easy to support new hardware via plugins for API endpoints and specific robot hardware
* Can be connected to `ROS2`, `Zenoh`, and `CycloneDDS`
* Includes a simple web-based debug display to watch the system work (`WebSim` at http://localhost:8000)
* Preconfigured endpoints for Voice-to-Speech, OpenAI's `gpt-4o`, DeepSeek, and multiple VLMs

<CardGroup cols={2}>
  <Card
    title="Quick Start"
    icon="check"
    href="./quick_start"
  >
    Get started with Openmind OS (OM1)
  </Card>
  <Card
    title="Developer Guide"
    icon="code"
    href="./development/guide"
  >
    Developer Guide for Openmind OS (OM1)
  </Card>
  <Card
    title="Unitree Robotics"
    icon="dog"
    href="./robotics/unitree_robotics"
  >
    Run OM1 on a Unitree Dog
  </Card>
</CardGroup>